{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos para identificar genes de resistência"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como construir o melhor bancos de dados?\n",
    "\n",
    "Não balanceado: 66k de sequências de proteínas vindas de genes não resistêntes + 17k de genes resistentes?\n",
    "\n",
    "Balanceado: tamanhos iguais para sequências positivas de negativa. Downsampling or upsampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dataset](/home/tiago/documents/PhD-Tiago/pics/Databaseconstruction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identificador de genes de resistência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training(negative_dataset:str,positive_dataset:str):\n",
    "    \"\"\"Load and prepare datases for model training, test and evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        negative_data:list -> proteins sequences that are not taken as antibiotic resistance proteins\n",
    "        negative_lavel:array-> a array of zeros representing non resistance label\n",
    "        positive_data:list -> resistance proteins sequences\n",
    "        positive_label:array -> a array of 1 representing resistance label\n",
    "    \"\"\"\n",
    "    #Loading nonargs data\n",
    "    negative_data = [str(info.seq) for info in SeqIO.parse(negative_dataset, \"fasta\")]\n",
    "    negative_data = list(map(\" \".join,negative_data))\n",
    "    negative_label = np.zeros((len(negative_data),1), float)\n",
    "    #Loading ARG data\n",
    "    positive_data = [str(info.seq) for info in SeqIO.parse(positive_dataset, \"fasta\")]\n",
    "    positive_data = list(map(\" \".join,positive_data))\n",
    "    positive_label = np.ones((len(positive_data),1), float)\n",
    "    return negative_data,negative_label,positive_data,positive_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_proteins_seq, neg_proteins_tags,pos_proteins_seq,pos_proteins_tags =load_training(\n",
    "    negative_dataset = \"datasets/uniprot/negative.db.fasta\",\n",
    "    positive_dataset = \"datasets/hmd/arg_v5.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X_p_sub = np.random.choice(pos_proteins_seq, size = int(len(pos_proteins_seq)*0.10), replace = False)\n",
    "y_p_sub = np.random.choice(pos_proteins_tags.ravel(), size = int(pos_proteins_tags.shape[0]*0.10), replace = False)\n",
    "X_n_sub = np.random.choice(neg_proteins_seq, size = int(len(neg_proteins_seq)*0.10), replace = False)\n",
    "y_n_sub = np.random.choice(neg_proteins_tags.ravel(), size = int(neg_proteins_tags.shape[0]*0.10), replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del neg_proteins_seq, neg_proteins_tags, pos_proteins_seq, pos_proteins_tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Samples split\n",
    "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X_p_sub,y_p_sub, random_state = 42)\n",
    "X_train_n, X_test_n, y_train_n, y_test_n = train_test_split(X_n_sub,y_n_sub, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_n_sub ,X_p_sub, y_n_sub, y_p_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merged samples\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    np.concatenate((X_train_n, X_train_p), axis = 0).reshape(-1,1),\n",
    "    np.concatenate((X_test_n, X_test_p), axis = 0).reshape(-1,1),\n",
    "    np.concatenate((y_train_n, y_train_p), axis = 0).reshape(-1,1),\n",
    "    np.concatenate((y_test_n, y_test_p), axis = 0).reshape(-1,1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_p, X_test_p, y_train_p, y_test_p, X_train_n, X_test_n, y_train_n, y_test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6281, 1), (2094, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information on protein sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "68699\n"
     ]
    }
   ],
   "source": [
    "#Vocabulary -> number of different amoniacids in my database\n",
    "aminoacids = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "longest_protein = sorted(list(map(len,list(X_train.ravel()))))[-1]\n",
    "print(len(aminoacids))\n",
    "print(aminoacids)\n",
    "print(longest_protein)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder_obj = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_encoder(array, classes_array,encoder_type):\n",
    "    \"\"\"\"\"\"\n",
    "    encoder_type.fit(classes_array)\n",
    "    encoded_proteins = []\n",
    "    for n in range(array.shape[0]):\n",
    "        encoded_proteins.append(list(encoder_obj.transform(array[n][0].split(\" \"))))\n",
    "    return encoded_proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = sequence_encoder(X_train,aminoacids,encoder_obj)\n",
    "X_train_padded = pad_sequences(X_train_padded, maxlen = longest_protein, padding='post',value=27) #sets the same lenght for all vector\n",
    "X_test_padded = sequence_encoder(X_test, aminoacids,encoder_obj)\n",
    "X_test_padded = pad_sequences(X_test_padded, maxlen = longest_protein, padding = \"post\", value = 27 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  0,  0, ..., 27, 27, 27], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del aminoacids, X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6281, 68699), (2094, 68699), (6281, 1), (2094, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded.shape, X_test_padded.shape,y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 68699, 1)          28        \n",
      "                                                                 \n",
      " conv1d_18 (Conv1D)          (None, 68540, 32)         5152      \n",
      "                                                                 \n",
      " max_pooling1d_12 (MaxPoolin  (None, 6854, 32)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_19 (Conv1D)          (None, 6735, 64)          245824    \n",
      "                                                                 \n",
      " conv1d_20 (Conv1D)          (None, 6616, 128)         983168    \n",
      "                                                                 \n",
      " max_pooling1d_13 (MaxPoolin  (None, 661, 128)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_21 (Conv1D)          (None, 602, 256)          1966336   \n",
      "                                                                 \n",
      " conv1d_22 (Conv1D)          (None, 543, 256)          3932416   \n",
      "                                                                 \n",
      " max_pooling1d_14 (MaxPoolin  (None, 135, 256)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_23 (Conv1D)          (None, 76, 256)           3932416   \n",
      "                                                                 \n",
      " max_pooling1d_15 (MaxPoolin  (None, 38, 256)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 9728)              0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 9728)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 12288)             119549952 \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1024)              12583936  \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 143,200,253\n",
      "Trainable params: 143,200,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "CNN = Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim = 26+2, output_dim = 1, input_length = longest_protein),\n",
    "\n",
    "    tf.keras.layers.Conv1D(filters = 32,kernel_size = 40*4),  #Conv1\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 5*2), \n",
    "\n",
    "    tf.keras.layers.Conv1D(filters = 64,kernel_size = 30*4),  #Conv2\n",
    "\n",
    "    tf.keras.layers.Conv1D(filters = 128,kernel_size = 30*4), #Conv3\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 5*2),\n",
    "\n",
    "    tf.keras.layers.Conv1D(filters = 256,kernel_size = 20*3), #Conv4\n",
    "\n",
    "\n",
    "    tf.keras.layers.Conv1D(filters = 256,kernel_size = 20*3), #Conv5\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 4),\n",
    "\n",
    "    tf.keras.layers.Conv1D(filters = 256,kernel_size = 20*3),    #Conv6\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.42),\n",
    "    \n",
    "    tf.keras.layers.Dense(12288, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(1024, activation = \"relu\"),\n",
    "\n",
    "    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "CNN.compile(\n",
    "    optimizer = tf.optimizers.Adam(),\n",
    "    loss = tf.losses.BinaryCrossentropy(),\n",
    "    metrics = [\n",
    "        tf.metrics.BinaryAccuracy(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall()\n",
    "    ]\n",
    ")\n",
    "print(CNN.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.5.3 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "trainer = CNN.fit(\n",
    "     X_train_padded,\n",
    "     y_train,\n",
    "     epochs = 1,\n",
    "     batch_size = 10,\n",
    "     validation_data = (X_test_padded,y_test),  \n",
    "     verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(plot_name:str):\n",
    "\n",
    "    #Plot loss\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.subplot(221)\n",
    "    plt.title('Loss')\n",
    "    plt.plot(trainer.history['loss'], label='train')\n",
    "    plt.plot(trainer.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    #Plot accuracy during training\n",
    "    plt.subplot(222)\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(trainer.history['binary_accuracy'], label='train')\n",
    "    plt.plot(trainer.history['val_binary_accuracy'], label='test')\n",
    "    plt.legend()\n",
    "    #Plot Precison\n",
    "    plt.subplot(223)\n",
    "    plt.title('Precison')\n",
    "    plt.plot(trainer.history['precision_1'], label='train')\n",
    "    plt.plot(trainer.history['val_precision_1'], label='test')\n",
    "    plt.legend()\n",
    "    #Plot Recall\n",
    "    plt.subplot(224)\n",
    "    plt.title('Recall')\n",
    "    plt.plot(trainer.history['recall_1'], label='train')\n",
    "    plt.plot(trainer.history['val_recall_1'], label='test')\n",
    "    plt.legend()\n",
    "    plt.savefig(plot_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = (CNN.predict(X_test_padded) > 0.5).astype(\"float\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_hat)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "sns.heatmap(cm, annot = True, fmt = \".5g\", cmap = \"coolwarm\",linewidths=.5)\n",
    "plt.ylabel('Actal Values',fontsize = 14)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.savefig(\"pics/identifier_cm.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_hat).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Precision = \\frac{tp}{tp+fp} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp/(tp+fp)\n",
    "print(f\"Precision: {np.round(precision,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Recall = \\frac{tp}{tp+fn} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp/(tp+fn)\n",
    "print(f\"Recall: {np.round(recall,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ F1-Score = 2\\frac{precision*recall}{precision+recall} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = np.round(2*((precision*recall)/(precision+recall)),2)\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os genes de resistência identificador vão passar por uma classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(array,integers):\n",
    "    encoder_obj = LabelEncoder()\n",
    "    encoder_obj.fit(integers)\n",
    "    encoded_proteins = []\n",
    "    for n in range(array.shape[0]):\n",
    "        encoded_proteins.append(list(encoder_obj.transform(array[n][0].split(\" \"))))\n",
    "    return encoded_proteins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificador sem alterações\n",
    "\n",
    "- Modelo com overfitting\n",
    "- 33 Classes\n",
    "- 1 Camada convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_proteins_seq =  [str(info.seq) for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\")]\n",
    "pos_proteins_seq = list(map(\" \".join,pos_proteins_seq))\n",
    "mechanisms = [info.description.split(\"|\")[5] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\")]\n",
    "classes = [info.description.split(\"|\")[3] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\")]\n",
    "classes_to_fit = list(set(classes))\n",
    "print(sorted(classes_to_fit))\n",
    "X_train, X_test, y_train, y_test = train_test_split(pos_proteins_seq,classes, random_state = 42)\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    np.array(X_train).reshape(-1,1),\n",
    "    np.array(X_test).reshape(-1,1),\n",
    "    np.array(y_train).reshape(-1,1),\n",
    "    np.array(y_test).reshape(-1,1)\n",
    ")\n",
    "longest_protein = sorted(list(map(len,list(X_train.ravel()))))[-1]\n",
    "print(longest_protein)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "aminoacids = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "X_train_encoded = label_encoder(X_train,aminoacids)\n",
    "X_train_padded = pad_sequences(X_train_encoded, maxlen = longest_protein, padding='post', value = 27) #sets the same lenght for all vector\n",
    "X_train_padded  =  np.asarray(X_train_padded).astype('float32')\n",
    "X_test_encoded = label_encoder(X_test,aminoacids)\n",
    "X_test_padded = pad_sequences(X_test_encoded, maxlen = longest_protein, padding='post', value = 27) #sets the same lenght for all vector\n",
    "print(X_train_padded.shape, X_test_padded.shape, y_train.shape, y_test.shape)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(classes_to_fit)\n",
    "y_train_encodded = encoder.transform(y_train)\n",
    "y_train_encodded = y_train_encodded.reshape(-1,1)\n",
    "y_test_encodded = encoder.transform(y_test)\n",
    "y_test_encodded = y_test_encodded.reshape(-1,1)\n",
    "classificador = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim = 26+2,output_dim = 8,input_length=longest_protein),\n",
    "    tf.keras.layers.Conv1D(filters = 32,kernel_size = 12),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 4),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    #tf.keras.layers.Dropout(0.42),\n",
    "    tf.keras.layers.Dense(len(classes_to_fit),activation = \"softmax\")\n",
    "    ])\n",
    "classificador.compile(\n",
    "    optimizer = tf.optimizers.Adam(),\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "    ]\n",
    ")\n",
    "print(classificador.summary())\n",
    "classes_models_trainer = classificador.fit(\n",
    "    X_train_padded,\n",
    "    y_train_encodded,\n",
    "    validation_data = (X_test_padded,y_test_encodded),\n",
    "    epochs = 40,\n",
    "    verbose = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_predicted(array,all_classes):\n",
    "    tranlated = []\n",
    "    for i in array:\n",
    "        tranlated.append((all_classes[np.where(i == np.max(i))[0][0]]))\n",
    "    return tranlated\n",
    "Y_hat_classes = classificador.predict(X_test_padded)\n",
    "Y_hat_classes_tran = translate_predicted(Y_hat_classes, classes_to_fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(y_test.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(y_train.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_train.ravel()).difference(y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(list(y_test.ravel()),Y_hat_classes_tran)\n",
    "cm_df = pd.DataFrame(cm,index = classes_to_fit,columns = classes_to_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "pd.Series(mechanisms).value_counts().plot.bar(color = [\"orange\",\"green\",\"blue\",\"red\",\"purple\"])\n",
    "plt.xticks(rotation = 30, ha = \"right\", fontsize = 14)\n",
    "plt.yticks(fontsize = 16)\n",
    "#plt.savefig(\"pics/mech_bar.png\",dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "pd.get_dummies(pd.DataFrame({\" \": mechanisms}, index = classes)).groupby(level = 0).sum().plot.barh(stacked = True,figsize=(18,12)).legend(bbox_to_anchor=(.42, 1),fontsize = 16).set_title(\"Mechanisms\")\n",
    "plt.title(\"Classes distribution\", fontsize = 20)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "#plt.savefig(\"pics/classes_dist.png\", dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Plot loss\n",
    "plt.figure(figsize = (8,12))\n",
    "plt.subplot(211)\n",
    "plt.plot(classes_models_trainer.history['loss'], label='train')\n",
    "plt.plot(classes_models_trainer.history['val_loss'], label='test')\n",
    "plt.title('Loss', fontsize = 24)\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.xlabel(\"epochs\",fontsize = 15)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.ylim(0,.6)\n",
    "plt.legend(fontsize = 18)\n",
    "#Plot accuracy during training\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy',fontsize = 24)\n",
    "plt.plot(classes_models_trainer.history['sparse_categorical_accuracy'], label='train')\n",
    "plt.plot(classes_models_trainer.history['val_sparse_categorical_accuracy'], label='test')\n",
    "plt.xlabel(\"epochs\",fontsize = 15)\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.ylim(.75,1)\n",
    "plt.legend(fontsize = 18)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"classes_metrics_14.04.2022.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificador - Funcionando\n",
    "\n",
    "- Número de classes reduzido para 15\n",
    "- Sem overfitting\n",
    "- 4 camadas convolucionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aminoglycoside', 'bacitracin', 'beta_lactam', 'chloramphenicol', 'fosfomycin', 'glycopeptide', 'macrolide-lincosamide-streptogramin', 'multidrug', 'polymyxin', 'quinolone', 'rifampin', 'sulfonamide', 'tetracycline', 'trimethoprim']\n",
      "3151\n",
      "(12797, 1) (4266, 1) (12797, 1) (4266, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'label_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tiago/documents/PhD-Tiago/prototipo.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bseriema.fcfrp.usp.br/home/tiago/documents/PhD-Tiago/prototipo.ipynb#ch0000007vscode-remote?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(X_train\u001b[39m.\u001b[39mshape, X_test\u001b[39m.\u001b[39mshape, y_train\u001b[39m.\u001b[39mshape, y_test\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bseriema.fcfrp.usp.br/home/tiago/documents/PhD-Tiago/prototipo.ipynb#ch0000007vscode-remote?line=21'>22</a>\u001b[0m aminoacids \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mABCDEFGHIJKLMNOPQRSTUVWXYZ\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bseriema.fcfrp.usp.br/home/tiago/documents/PhD-Tiago/prototipo.ipynb#ch0000007vscode-remote?line=22'>23</a>\u001b[0m X_train_encoded \u001b[39m=\u001b[39m label_encoder(X_train,aminoacids)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bseriema.fcfrp.usp.br/home/tiago/documents/PhD-Tiago/prototipo.ipynb#ch0000007vscode-remote?line=23'>24</a>\u001b[0m X_train_padded \u001b[39m=\u001b[39m pad_sequences(X_train_encoded, maxlen \u001b[39m=\u001b[39m longest_protein, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m'\u001b[39m, value \u001b[39m=\u001b[39m \u001b[39m27\u001b[39m) \u001b[39m#sets the same lenght for all vector\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bseriema.fcfrp.usp.br/home/tiago/documents/PhD-Tiago/prototipo.ipynb#ch0000007vscode-remote?line=24'>25</a>\u001b[0m X_train_padded  \u001b[39m=\u001b[39m  np\u001b[39m.\u001b[39masarray(X_train_padded)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "classes = [info.description.split(\"|\")[3] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\")]\n",
    "mechanisms = [info.description.split(\"|\")[5] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\")]\n",
    "main_classes = pd.Series(classes).value_counts()[:14].sort_values(ascending = False)\n",
    "pos_proteins_seq =  [str(info.seq) for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\") if info.description.split(\"|\")[3] in main_classes]\n",
    "pos_proteins_seq = list(map(\" \".join,pos_proteins_seq))\n",
    "classes = [info.description.split(\"|\")[3] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\") if info.description.split(\"|\")[3] in main_classes]\n",
    "mechanisms = [info.description.split(\"|\")[5] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\") if info.description.split(\"|\")[3] in main_classes]\n",
    "classes_to_fit = list(set(classes))\n",
    "print(sorted(classes_to_fit))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(pos_proteins_seq,classes, random_state = 42)\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    np.array(X_train).reshape(-1,1),\n",
    "    np.array(X_test).reshape(-1,1),\n",
    "    np.array(y_train).reshape(-1,1),\n",
    "    np.array(y_test).reshape(-1,1)\n",
    ")\n",
    "\n",
    "longest_protein = sorted(list(map(len,list(X_train.ravel()))))[-1]\n",
    "print(longest_protein)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "aminoacids = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "X_train_encoded = label_encoder(X_train,aminoacids)\n",
    "X_train_padded = pad_sequences(X_train_encoded, maxlen = longest_protein, padding='post', value = 27) #sets the same lenght for all vector\n",
    "X_train_padded  =  np.asarray(X_train_padded).astype('float32')\n",
    "X_test_encoded = label_encoder(X_test,aminoacids)\n",
    "X_test_padded = pad_sequences(X_test_encoded, maxlen = longest_protein, padding='post', value = 27) #sets the same lenght for all vector\n",
    "print(X_train_padded.shape, X_test_padded.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(classes_to_fit)\n",
    "y_train_encodded = encoder.transform(y_train)\n",
    "y_train_encodded = y_train_encodded.reshape(-1,1)\n",
    "y_test_encodded = encoder.transform(y_test)\n",
    "y_test_encodded = y_test_encodded.reshape(-1,1)\n",
    "classificador_train = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim = 26+2,output_dim = 8,input_length=longest_protein),\n",
    "    tf.keras.layers.Conv1D(filters = 64,kernel_size = 24),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 8),\n",
    "    tf.keras.layers.Dropout(0.42),\n",
    "    tf.keras.layers.Conv1D(filters = 32,kernel_size = 12),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 4),\n",
    "    tf.keras.layers.Dropout(0.42),\n",
    "    tf.keras.layers.Conv1D(filters = 16,kernel_size = 6),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 2),\n",
    "    tf.keras.layers.Dropout(0.42),\n",
    "    tf.keras.layers.Conv1D(filters = 8,kernel_size = 4),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 1),   \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.42),\n",
    "    tf.keras.layers.Dense(len(classes_to_fit),activation = \"softmax\")\n",
    "    ])\n",
    "classificador_train.compile(\n",
    "    optimizer = tf.optimizers.Adam(),\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "    ]\n",
    ")\n",
    "print(classificador_train.summary())\n",
    "classes_models_trainer = classificador_train.fit(\n",
    "    X_train_padded,\n",
    "    y_train_encodded,\n",
    "    validation_data = (X_test_padded,y_test_encodded),\n",
    "    epochs = 5,\n",
    "    verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot loss\n",
    "plt.figure(figsize = (8,12))\n",
    "plt.subplot(211)\n",
    "plt.plot(classes_models_trainer.history['loss'], label='train')\n",
    "plt.plot(classes_models_trainer.history['val_loss'], label='test')\n",
    "plt.title('Loss', fontsize = 24)\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.xlabel(\"epochs\",fontsize = 15)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.ylim(0,.6)\n",
    "plt.legend(fontsize = 18)\n",
    "#Plot accuracy during training\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy',fontsize = 24)\n",
    "plt.plot(classes_models_trainer.history['sparse_categorical_accuracy'], label='train')\n",
    "plt.plot(classes_models_trainer.history['val_sparse_categorical_accuracy'], label='test')\n",
    "plt.xlabel(\"epochs\",fontsize = 15)\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.ylim(.75,1)\n",
    "plt.legend(fontsize = 18)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"Main_classes_metrics_18.04.2022.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(classes_to_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_predicted(array,all_classes):\n",
    "    tranlated = []\n",
    "    for i in array:\n",
    "        tranlated.append((all_classes[np.where(i == np.max(i))[0][0]]))\n",
    "    return tranlated\n",
    "    \n",
    "Y_hat_classes_tran = translate_predicted(Y_hat_classes, list(encoder.classes_))\n",
    "cm = confusion_matrix(list(y_test.ravel()),Y_hat_classes_tran)\n",
    "cm_df = pd.DataFrame(cm,index = list(encoder.classes_),columns = list(encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm_df, annot=True, fmt = \".4g\", linewidths=.5, cmap=\"coolwarm\")\n",
    "plt.title('Confusion Matrix - Classes',fontsize = 28)\n",
    "plt.ylabel('Actual Values',fontsize = 18)\n",
    "plt.xlabel('Predicted Values',fontsize = 18)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "#plt.savefig(\"pics/cm_classes_main_classes.png\", dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "pd.get_dummies(pd.DataFrame({\" \": mechanisms}, index = classes)).groupby(level = 0).sum().plot.barh(stacked = True,figsize=(18,12)).legend(fontsize = 16).set_title(\"Mechanisms\")\n",
    "plt.title(\"Classes distribution\", fontsize = 20)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "plt.savefig(\"pics/main_classes_dist.png\", dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "pd.Series(mechanisms).value_counts().plot.bar(color = [\"orange\",\"green\",\"blue\",\"red\",\"purple\"])\n",
    "plt.xticks(rotation = 30, ha = \"right\", fontsize = 14)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.savefig(\"pics/main_classes_mech_bar.png\",dpi = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [info.description.split(\"|\")[3] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\")]\n",
    "mechanisms = [info.description.split(\"|\")[5] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\")]\n",
    "main_classes = pd.Series(classes).value_counts()[:14].sort_values(ascending = False)\n",
    "pos_proteins_seq =  [str(info.seq) for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\") if info.description.split(\"|\")[3] in main_classes]\n",
    "pos_proteins_seq = list(map(\" \".join,pos_proteins_seq))\n",
    "classes = [info.description.split(\"|\")[3] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\") if info.description.split(\"|\")[3] in main_classes]\n",
    "mechanisms = [info.description.split(\"|\")[5] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\") if info.description.split(\"|\")[3] in main_classes]\n",
    "classes_to_fit = list(set(classes))\n",
    "print(sorted(classes_to_fit))\n",
    "X = np.array(pos_proteins_seq).reshape(-1,1)\n",
    "y = np.array(classes).reshape(-1,1)\n",
    "longest_protein = sorted(list(map(len,list(X.ravel()))))[-1]\n",
    "print(longest_protein)\n",
    "aminoacids = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "X_encoded = label_encoder(X,aminoacids)\n",
    "X_padded  = pad_sequences(X_encoded, maxlen = longest_protein, padding='post', value = 27) #sets the same lenght for all vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_padded  =  np.asarray(X_padded).astype('float32')\n",
    "print(X_train_padded.shape, X_test_padded.shape, y_train.shape, y_test.shape)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(classes_to_fit)\n",
    "y_encodded = encoder.transform(y)\n",
    "y_encodded = y_encodded.reshape(-1,1)\n",
    "\n",
    "classificador = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim = 26+2,output_dim = 8,input_length=longest_protein),\n",
    "    tf.keras.layers.Conv1D(filters = 64,kernel_size = 24),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 8),\n",
    "    tf.keras.layers.Dropout(0.42),\n",
    "    tf.keras.layers.Conv1D(filters = 32,kernel_size = 12),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 4),\n",
    "    tf.keras.layers.Dropout(0.42),\n",
    "    tf.keras.layers.Conv1D(filters = 16,kernel_size = 6),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 2),\n",
    "    tf.keras.layers.Dropout(0.42),\n",
    "    tf.keras.layers.Conv1D(filters = 8,kernel_size = 4),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 1),   \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.42),\n",
    "    tf.keras.layers.Dense(len(classes_to_fit),activation = \"softmax\")\n",
    "    ])\n",
    "classificador.compile(\n",
    "    optimizer = tf.optimizers.Adam(),\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "    ]\n",
    ")\n",
    "print(classificador.summary())\n",
    "classes_models_trainer = classificador.fit(\n",
    "    X_padded,\n",
    "    y_encodded,\n",
    "    validation_data = (X_test_padded,y_test_encodded),\n",
    "    epochs = 40,\n",
    "    verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador.save(\"trained_models/classifier_production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificador Overbalanced\n",
    "\n",
    "- Balanceamentos das classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [info.description.split(\"|\")[3] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\")]\n",
    "mechanisms = [info.description.split(\"|\")[5] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\")]\n",
    "main_classes = pd.Series(classes).value_counts()[:14].sort_values(ascending = False)\n",
    "pos_proteins_seq =  [str(info.seq) for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\") if info.description.split(\"|\")[3] in main_classes]\n",
    "pos_proteins_seq = list(map(\" \".join,pos_proteins_seq))\n",
    "classes = [info.description.split(\"|\")[3] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\") if info.description.split(\"|\")[3] in main_classes]\n",
    "mechanisms = [info.description.split(\"|\")[5] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\") if info.description.split(\"|\")[3] in main_classes]\n",
    "classes_to_fit = list(set(classes))\n",
    "print(sorted(classes_to_fit))\n",
    "X_train, X_test, y_train, y_test = train_test_split(pos_proteins_seq,classes, random_state = 42)\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    np.array(X_train).reshape(-1,1),\n",
    "    np.array(X_test).reshape(-1,1),\n",
    "    np.array(y_train).reshape(-1,1),\n",
    "    np.array(y_test).reshape(-1,1)\n",
    ")\n",
    "longest_protein = sorted(list(map(len,list(X_train.ravel()))))[-1]\n",
    "print(longest_protein)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "aminoacids = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "X_train_encoded = label_encoder(X_train,aminoacids)\n",
    "X_train_padded = pad_sequences(X_train_encoded, maxlen = longest_protein, padding='post', value = 27) #sets the same lenght for all vector\n",
    "X_train_padded  =  np.asarray(X_train_padded).astype('float32')\n",
    "X_test_encoded = label_encoder(X_test,aminoacids)\n",
    "X_test_padded = pad_sequences(X_test_encoded, maxlen = longest_protein, padding='post', value = 27) #sets the same lenght for all vector\n",
    "print(X_train_padded.shape, X_test_padded.shape, y_train.shape, y_test.shape)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(classes_to_fit)\n",
    "y_train_encodded = encoder.transform(y_train)\n",
    "y_train_encodded = y_train_encodded.reshape(-1,1)\n",
    "y_test_encodded = encoder.transform(y_test)\n",
    "y_test_encodded = y_test_encodded.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before balance\n",
    "print(sorted(Counter(y_train_encodded.ravel()).items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling\n",
    "oversampler = RandomOverSampler(random_state = 0)\n",
    "X_train_padded, y_train_encodded =  SMOTE().fit_resample(X_train_padded,y_train_encodded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After oversampling\n",
    "print(sorted(Counter(y_train_encodded.ravel()).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim = 26+2,output_dim = 8,input_length=longest_protein),\n",
    "    tf.keras.layers.Conv1D(filters = 32,kernel_size = 12),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 4),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.42),\n",
    "    tf.keras.layers.Dense(len(classes_to_fit),activation = \"softmax\")\n",
    "    ])\n",
    "classificador.compile(\n",
    "    optimizer = tf.optimizers.Adam(),\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "    ]\n",
    ")\n",
    "print(classificador.summary())\n",
    "classes_models_trainer = classificador.fit(\n",
    "    X_train_padded,\n",
    "    y_train_encodded,\n",
    "    validation_data = (X_test_padded,y_test_encodded),\n",
    "    epochs = 40,\n",
    "    verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot loss\n",
    "plt.figure(figsize = (8,12))\n",
    "plt.subplot(211)\n",
    "plt.plot(classes_models_trainer.history['loss'], label='train')\n",
    "plt.plot(classes_models_trainer.history['val_loss'], label='test')\n",
    "plt.title('Loss', fontsize = 24)\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.xlabel(\"epochs\",fontsize = 15)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.ylim(0,.6)\n",
    "plt.legend(fontsize = 18)\n",
    "#Plot accuracy during training\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy',fontsize = 24)\n",
    "plt.plot(classes_models_trainer.history['sparse_categorical_accuracy'], label='train')\n",
    "plt.plot(classes_models_trainer.history['val_sparse_categorical_accuracy'], label='test')\n",
    "plt.xlabel(\"epochs\",fontsize = 15)\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.ylim(.75,1)\n",
    "plt.legend(fontsize = 18)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"Main_classes_metrics_18.04.2022.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat_classes = classificador.predict(X_test_padded)\n",
    "Y_hat_classes_tran = translate_predicted(Y_hat_classes, classes_to_fit)\n",
    "cm = confusion_matrix(list(y_test.ravel()),Y_hat_classes_tran)\n",
    "cm_df = pd.DataFrame(cm,index = classes_to_fit,columns = classes_to_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm_df, annot=True, fmt = \".4g\", linewidths=.5, cmap=\"coolwarm\")\n",
    "plt.title('Confusion Matrix - Classes',fontsize = 28)\n",
    "plt.ylabel('Actual Values',fontsize = 18)\n",
    "plt.xlabel('Predicted Values',fontsize = 18)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "#plt.savefig(\"pics/cm_classes_main_classes.png\", dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mecanismos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_proteins_seq =  [str(info.seq) for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\")]\n",
    "pos_proteins_seq = list(map(\" \".join,pos_proteins_seq))\n",
    "mechanisms = [info.description.split(\"|\")[5] for info in SeqIO.parse(\"datasets/hmd/arg_v5.fasta\", \"fasta\")]\n",
    "mechanisms_to_fit = list(set(mechanisms))\n",
    "print(sorted(mechanisms_to_fit))\n",
    "X_train, X_test, y_train, y_test = train_test_split(pos_proteins_seq,mechanisms, random_state = 42)\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    np.array(X_train).reshape(-1,1),\n",
    "    np.array(X_test).reshape(-1,1),\n",
    "    np.array(y_train).reshape(-1,1),\n",
    "    np.array(y_test).reshape(-1,1)\n",
    ")\n",
    "longest_protein = sorted(list(map(len,list(X_train.ravel()))))[-1]\n",
    "print(longest_protein)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "aminoacids = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "X_train_encoded = label_encoder(X_train,aminoacids)\n",
    "X_train_padded = pad_sequences(X_train_encoded, maxlen = longest_protein, padding='post', value = 27) #sets the same lenght for all vector\n",
    "X_train_padded  =  np.asarray(X_train_padded).astype('float32')\n",
    "X_test_encoded = label_encoder(X_test,aminoacids)\n",
    "X_test_padded = pad_sequences(X_test_encoded, maxlen = longest_protein, padding='post', value = 27) #sets the same lenght for all vector\n",
    "print(X_train_padded.shape, X_test_padded.shape, y_train.shape, y_test.shape)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(mechanisms_to_fit)\n",
    "y_train_encodded = encoder.transform(y_train)\n",
    "y_train_encodded = y_train_encodded.reshape(-1,1)\n",
    "y_test_encodded = encoder.transform(y_test)\n",
    "y_test_encodded = y_test_encodded.reshape(-1,1)\n",
    "classificador = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim = 26+2,output_dim = 8,input_length=longest_protein),\n",
    "    tf.keras.layers.Conv1D(filters = 32,kernel_size = 12),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size = 4),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.42),\n",
    "    tf.keras.layers.Dense(len(mechanisms_to_fit),activation = \"softmax\")\n",
    "    ])\n",
    "classificador.compile(\n",
    "    optimizer = tf.optimizers.Adam(),\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "    ]\n",
    ")\n",
    "print(classificador.summary())\n",
    "mech_models_trainer = classificador.fit(\n",
    "    X_train_padded,\n",
    "    y_train_encodded,\n",
    "    validation_data = (X_test_padded,y_test_encodded),\n",
    "    epochs = 40,\n",
    "    verbose = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_predicted(array,all_classes):\n",
    "    tranlated = []\n",
    "    for i in array:\n",
    "        tranlated.append((all_classes[np.where(i == np.max(i))[0][0]]))\n",
    "    return tranlated\n",
    "Y_hat_mech = classificador.predict(X_test_padded)\n",
    "Y_hat_mech_tran = translate_predicted(Y_hat_mech, mechanisms_to_fit)\n",
    "cm = confusion_matrix(list(y_test.ravel()),Y_hat_mech_tran)\n",
    "cm_df = pd.DataFrame(cm,index = mechanisms_to_fit,columns = mechanisms_to_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm_df, annot=True, fmt = \".4g\", linewidths=.5, cmap=\"coolwarm\")\n",
    "plt.title('Confusion Matrix - Mechanisms',fontsize = 28)\n",
    "plt.ylabel('Actual Values',fontsize = 18)\n",
    "plt.xlabel('Predicted Values',fontsize = 18)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.savefig(\"pics/cm_mechanisms.png\", dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Plot loss\n",
    "plt.figure(figsize = (8,12))\n",
    "plt.subplot(211)\n",
    "plt.plot(mech_models_trainer.history['loss'], label='train')\n",
    "plt.plot(mech_models_trainer.history['val_loss'], label='test')\n",
    "plt.title('Loss', fontsize = 24)\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.xlabel(\"epochs\",fontsize = 15)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.ylim(0,.6)\n",
    "plt.legend(fontsize = 18)\n",
    "#Plot accuracy during training\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy',fontsize = 24)\n",
    "plt.plot(mech_models_trainer.history['sparse_categorical_accuracy'], label='train')\n",
    "plt.plot(mech_models_trainer.history['val_sparse_categorical_accuracy'], label='test')\n",
    "plt.xlabel(\"epochs\",fontsize = 15)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.ylim(.75,1)\n",
    "plt.legend(fontsize = 18)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"mechamisms_metrics_14.04.2022.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador.save(\"trained_models/model_mechanisms.25.03.2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = Data.Data(\n",
    "    negative_dataset = \"datasets/uniprot/negative.db.fasta\",\n",
    "    positive_dataset = \"datasets/hmd/arg_v5.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_proteins_seq, neg_proteins_tags,pos_proteins_seq,pos_proteins_tags = my_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_data.get_longest_protein()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_set_data = Data.Subset_data(\n",
    "    negative_dataset = \"datasets/uniprot/negative.db.fasta\",\n",
    "    positive_dataset = \"datasets/hmd/arg_v5.fasta\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,c,b,d  = sub_set_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6647"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<U70425\n"
     ]
    }
   ],
   "source": [
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<U6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array(['apple', 'banana', 'cherry'])\n",
    "\n",
    "print(arr.dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
